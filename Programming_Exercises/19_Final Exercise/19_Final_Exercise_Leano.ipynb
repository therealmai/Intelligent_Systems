{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZcv4vg7Tr0U"
      },
      "source": [
        "## UNDERSTANDING DEEP LEARNING REQUIRES RE-THINKING GENERALIZATION\n",
        "### https://arxiv.org/pdf/1611.03530.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "awTg-UgMTr0e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8IO35089Tr0h"
      },
      "outputs": [],
      "source": [
        "input_channels = 3\n",
        "num_classes = 10\n",
        "\n",
        "device = 'cuda'\n",
        "lr = 0.01\n",
        "epochs = 80\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlM8roZgTr0i"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yMORsbErTr0j"
      },
      "outputs": [],
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # convert image to tensor\n",
        "    transforms.CenterCrop(28) # crop from the center\n",
        "])\n",
        "\n",
        "def norm_image(data_sample):\n",
        "    img_tensor = data_sample[0] # image tensors\n",
        "    label = data_sample[1]\n",
        "\n",
        "    img_means = img_tensor.mean(axis=[1,2]) # mean of image per channel\n",
        "    img_sds = img_tensor.std(axis=[1,2]) # overall SD of image per channel\n",
        "\n",
        "    mean_sub = img_tensor - img_means.unsqueeze(1).unsqueeze(2)\n",
        "    img_norm = mean_sub.true_divide(img_sds.unsqueeze(1).unsqueeze(2))\n",
        "\n",
        "    return (img_norm, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzvvEA-nTr0j",
        "outputId": "7a935f76-babd-4281-8a3f-8bf9b15f1aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# training set\n",
        "\n",
        "all_train = list(datasets.CIFAR10(root = 'data/', transform=img_transform, train = True, download=True))\n",
        "\n",
        "random.shuffle(all_train)\n",
        "\n",
        "train_data = all_train[:40000]\n",
        "train_transformed = list(map(norm_image, train_data))\n",
        "train_loader = DataLoader(dataset=train_transformed, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_data = all_train[40000:]\n",
        "val_transformed = list(map(norm_image, val_data))\n",
        "val_loader = DataLoader(dataset=val_transformed, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data = datasets.CIFAR10(root='data/', transform=img_transform, train=False, download=True)\n",
        "test_transformed = list(map(norm_image, val_data))\n",
        "test_loader = DataLoader(dataset=val_transformed, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzV2W-sATr0v"
      },
      "source": [
        "## Implementation of Reduced GoogLeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ffb2hjuATr0w"
      },
      "outputs": [],
      "source": [
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.relu = nn.ReLU() # ReLU\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs) # convolution\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) # batch normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class inception_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_ch1, out_ch3):\n",
        "        super(inception_block, self).__init__()\n",
        "        # first conv module, same padding\n",
        "        self.ch1 = conv_block(in_channels=in_channels, out_channels=out_ch1, kernel_size=(3,3), stride=(1,1), padding='same')\n",
        "        # second conv module, same padding\n",
        "        self.ch3 = conv_block(in_channels=in_channels, out_channels=out_ch3, kernel_size=(3,3), stride=(1,1), padding='same')\n",
        "\n",
        "    def forward(self,x):\n",
        "        # concatenate convolution outputs\n",
        "        return torch.cat([self.ch1(x),self.ch3(x)],1)\n",
        "\n",
        "class downsample_block(nn.Module):\n",
        "    def __init__(self, in_channels, conv_out):\n",
        "        super(downsample_block, self).__init__()\n",
        "        # conv module\n",
        "        self.convblock = conv_block(in_channels, conv_out, kernel_size=(3,3), stride=(2,2))\n",
        "        # max pooling\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(3,3), stride=(2,2))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return torch.cat([self.convblock(x),self.maxpool(x)],1)\n",
        "\n",
        "class mini_GoogLeNet(nn.Module):\n",
        "    def __init__(self,in_channels=3, num_classes=10, dropout_prob=0):\n",
        "        super(mini_GoogLeNet, self).__init__()\n",
        "        self.conv1 = conv_block(in_channels=3, out_channels=96, kernel_size=(3,3), stride=(1,1))\n",
        "        self.inception1 = inception_block(96,32,32)\n",
        "        self.inception2 = inception_block(64,32,48)\n",
        "        self.downsample1 = downsample_block(80,80)\n",
        "        self.inception3 = inception_block(160,112,48)\n",
        "        self.inception4 = inception_block(160,96,64)\n",
        "        self.inception5 = inception_block(160,80,80)\n",
        "        self.inception6 = inception_block(160,48,96)\n",
        "        self.downsample2 = downsample_block(144,96)\n",
        "        self.inception7 = inception_block(240,176,160)\n",
        "        self.inception8 = inception_block(336,176,160)\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=(7,7), padding=(1,1))\n",
        "        self.dropout = nn.Dropout(p = dropout_prob)\n",
        "        self.fc = nn.Linear(336,10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.inception1(x)\n",
        "        x = self.inception2(x)\n",
        "        x = self.downsample1(x)\n",
        "        x = self.inception3(x)\n",
        "        x = self.inception4(x)\n",
        "        x = self.inception5(x)\n",
        "        x = self.inception6(x)\n",
        "        x = self.downsample2(x)\n",
        "        x = self.inception7(x)\n",
        "        x = self.inception8(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0],-1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46dzdgGcTr0w"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FFbQC9KVTr0x"
      },
      "outputs": [],
      "source": [
        "model = mini_GoogLeNet(in_channels=3, num_classes=10, dropout_prob=0).to(device=device)\n",
        "\n",
        "# parameter settings\n",
        "loss_function = nn.CrossEntropyLoss() # loss funtion\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr) # optimizer\n",
        "scheduler = optim.lr_scheduler.LinearLR(optimizer) # scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zc9jlRd2Tr0x"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    curr_loss_train = 0 # loss\n",
        "    correct_train = 0 # correctly classified training points\n",
        "    total_train = 0 # total number of training points\n",
        "\n",
        "    for ind, (data_train, true_labels_train) in enumerate(train_loader):\n",
        "        data_train = data_train.to(device=device) # use GPU\n",
        "        true_labels_train = true_labels_train.to(device=device) # use GPU\n",
        "\n",
        "        out_train = model(data_train) # apply model to training data\n",
        "        loss_train = loss_function(out_train, true_labels_train) # get loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        curr_loss_train += loss_train.item()\n",
        "        ix, predicted_train = out_train.max(1)\n",
        "        correct_train += predicted_train.eq(true_labels_train).sum().item()\n",
        "        total_train += true_labels_train.size(0)\n",
        "\n",
        "    train_loss = curr_loss_train/len(train_loader) # get loss\n",
        "    acc_train_val = (correct_train/total_train)*100 # get accuracy\n",
        "\n",
        "    train_acc.append(acc_train_val)\n",
        "    train_all_loss.append(train_loss)\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    curr_loss_test = 0 # loss\n",
        "    correct_test = 0 # correctly classified test points\n",
        "    total_test = 0 # total number of test points\n",
        "\n",
        "    num_class = 10\n",
        "    confusion_matrix = torch.zeros(num_class, num_class)\n",
        "    with torch.no_grad():\n",
        "        for data_test, true_labels_test in test_loader:\n",
        "\n",
        "            data_test = data_test.to(device=device) # use GPU\n",
        "            true_labels_test = true_labels_test.to(device=device) # use GPU\n",
        "\n",
        "            out_test = model(data_test) # apply model to training data\n",
        "            loss_test = loss_function(out_test, true_labels_test) # get loss\n",
        "\n",
        "            # metrics\n",
        "            curr_loss_test += loss_test.item()\n",
        "            ix, predicted_test = out_test.max(1)\n",
        "            correct_test += predicted_test.eq(true_labels_test).sum().item()\n",
        "            total_test += true_labels_test.size(0)\n",
        "\n",
        "\n",
        "    test_loss = curr_loss_test/len(test_loader) # get loss\n",
        "    acc_test_val = (correct_test/total_test)*100 # get accuracy\n",
        "\n",
        "    test_acc.append(acc_test_val)\n",
        "    test_all_loss.append(test_loss)\n",
        "    con_mats.append(confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize results\n",
        "train_acc = []\n",
        "train_all_loss = []\n",
        "\n",
        "test_acc = []\n",
        "test_all_loss = []\n",
        "\n",
        "con_mats = []\n",
        "\n",
        "times = []\n",
        "train_times = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjF4y3LATr0y",
        "outputId": "b110d843-81f6-45be-c2f6-b43b78753866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Epoch time: 36.52 seconds\n",
            "Epoch 1\n",
            "Epoch time: 36.22 seconds\n",
            "Epoch 2\n",
            "Epoch time: 35.11 seconds\n",
            "Epoch 3\n",
            "Epoch time: 35.60 seconds\n",
            "Epoch 4\n",
            "Epoch time: 35.77 seconds\n",
            "Epoch 5\n",
            "Epoch time: 35.14 seconds\n",
            "Epoch 6\n",
            "Epoch time: 35.65 seconds\n",
            "Epoch 7\n",
            "Epoch time: 35.86 seconds\n",
            "Epoch 8\n",
            "Epoch time: 35.02 seconds\n",
            "Epoch 9\n",
            "Epoch time: 35.48 seconds\n",
            "Epoch 10\n",
            "Epoch time: 35.99 seconds\n",
            "Epoch 11\n",
            "Epoch time: 35.22 seconds\n",
            "Epoch 12\n",
            "Epoch time: 35.55 seconds\n",
            "Epoch 13\n",
            "Epoch time: 35.89 seconds\n",
            "Epoch 14\n",
            "Epoch time: 35.22 seconds\n",
            "Epoch 15\n",
            "Epoch time: 35.55 seconds\n",
            "Epoch 16\n",
            "Epoch time: 35.89 seconds\n",
            "Epoch 17\n",
            "Epoch time: 35.06 seconds\n",
            "Epoch 18\n",
            "Epoch time: 34.92 seconds\n",
            "Epoch 19\n",
            "Epoch time: 35.00 seconds\n",
            "Epoch 20\n",
            "Epoch time: 34.86 seconds\n",
            "Epoch 21\n",
            "Epoch time: 34.96 seconds\n",
            "Epoch 22\n",
            "Epoch time: 34.83 seconds\n",
            "Epoch 23\n",
            "Epoch time: 34.84 seconds\n",
            "Epoch 24\n",
            "Epoch time: 35.15 seconds\n",
            "Epoch 25\n",
            "Epoch time: 35.42 seconds\n",
            "Epoch 26\n",
            "Epoch time: 35.20 seconds\n",
            "Epoch 27\n",
            "Epoch time: 34.77 seconds\n",
            "Epoch 28\n",
            "Epoch time: 34.83 seconds\n",
            "Epoch 29\n",
            "Epoch time: 34.89 seconds\n",
            "Epoch 30\n",
            "Epoch time: 35.08 seconds\n",
            "Epoch 31\n",
            "Epoch time: 34.91 seconds\n",
            "Epoch 32\n",
            "Epoch time: 34.94 seconds\n",
            "Epoch 33\n",
            "Epoch time: 34.97 seconds\n",
            "Epoch 34\n",
            "Epoch time: 35.27 seconds\n",
            "Epoch 35\n",
            "Epoch time: 35.35 seconds\n",
            "Epoch 36\n",
            "Epoch time: 35.04 seconds\n",
            "Epoch 37\n",
            "Epoch time: 34.76 seconds\n",
            "Epoch 38\n",
            "Epoch time: 35.13 seconds\n",
            "Epoch 39\n",
            "Epoch time: 34.88 seconds\n",
            "Epoch 40\n",
            "Epoch time: 34.85 seconds\n",
            "Epoch 41\n",
            "Epoch time: 34.85 seconds\n",
            "Epoch 42\n",
            "Epoch time: 35.08 seconds\n",
            "Epoch 43\n",
            "Epoch time: 35.09 seconds\n",
            "Epoch 44\n",
            "Epoch time: 35.15 seconds\n",
            "Epoch 45\n",
            "Epoch time: 35.02 seconds\n",
            "Epoch 46\n",
            "Epoch time: 34.84 seconds\n",
            "Epoch 47\n",
            "Epoch time: 34.79 seconds\n",
            "Epoch 48\n",
            "Epoch time: 34.98 seconds\n",
            "Epoch 49\n",
            "Epoch time: 34.87 seconds\n",
            "Epoch 50\n",
            "Epoch time: 34.93 seconds\n",
            "Epoch 51\n",
            "Epoch time: 34.87 seconds\n",
            "Epoch 52\n",
            "Epoch time: 35.30 seconds\n",
            "Epoch 53\n",
            "Epoch time: 35.44 seconds\n",
            "Epoch 54\n",
            "Epoch time: 35.37 seconds\n",
            "Epoch 55\n",
            "Epoch time: 34.88 seconds\n",
            "Epoch 56\n",
            "Epoch time: 34.84 seconds\n",
            "Epoch 57\n",
            "Epoch time: 34.88 seconds\n",
            "Epoch 58\n",
            "Epoch time: 34.91 seconds\n",
            "Epoch 59\n",
            "Epoch time: 35.03 seconds\n",
            "Epoch 60\n",
            "Epoch time: 35.00 seconds\n",
            "Epoch 61\n",
            "Epoch time: 34.94 seconds\n",
            "Epoch 62\n",
            "Epoch time: 35.12 seconds\n",
            "Epoch 63\n",
            "Epoch time: 35.39 seconds\n",
            "Epoch 64\n",
            "Epoch time: 35.03 seconds\n",
            "Epoch 65\n",
            "Epoch time: 34.73 seconds\n",
            "Epoch 66\n",
            "Epoch time: 34.98 seconds\n",
            "Epoch 67\n",
            "Epoch time: 35.13 seconds\n",
            "Epoch 68\n",
            "Epoch time: 34.97 seconds\n",
            "Epoch 69\n",
            "Epoch time: 34.94 seconds\n",
            "Epoch 70\n",
            "Epoch time: 35.03 seconds\n",
            "Epoch 71\n",
            "Epoch time: 35.18 seconds\n",
            "Epoch 72\n",
            "Epoch time: 35.31 seconds\n",
            "Epoch 73\n",
            "Epoch time: 35.31 seconds\n",
            "Epoch 74\n",
            "Epoch time: 34.99 seconds\n",
            "Epoch 75\n",
            "Epoch time: 34.85 seconds\n",
            "Epoch 76\n",
            "Epoch time: 34.96 seconds\n",
            "Epoch 77\n",
            "Epoch time: 34.90 seconds\n",
            "Epoch 78\n",
            "Epoch time: 34.89 seconds\n",
            "Epoch 79\n",
            "Epoch time: 34.96 seconds\n",
            "\n",
            "Total training time: 2811.29 seconds\n"
          ]
        }
      ],
      "source": [
        "train_start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    ep_start = time.time()\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_time = time.time() - ep_start\n",
        "    print(f\"Epoch time: {epoch_time:0.2f} seconds\")\n",
        "    times.append(epoch_time)\n",
        "\n",
        "    train_time = time.time() - train_start\n",
        "    train_times.append(train_time)\n",
        "\n",
        "print()\n",
        "print(f\"Total training time: {train_time:0.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e0AQ1ahTr0y"
      },
      "source": [
        "## Exporting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "owKPEEyrTr0y"
      },
      "outputs": [],
      "source": [
        "df_res = pd.DataFrame()\n",
        "df_res['TrainAccuracy'] = train_acc\n",
        "df_res['TrainLoss'] = train_all_loss\n",
        "df_res['TestAccuracy'] = test_acc\n",
        "df_res['TestLoss'] = test_all_loss\n",
        "df_res['EpochTime'] = times\n",
        "df_res['Totaltime'] = train_times\n",
        "\n",
        "df_res.to_csv('19-results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u5vL6_pYTr0z"
      },
      "outputs": [],
      "source": [
        "with open('19-confusion_matrices.pickle','wb') as handle:\n",
        "    pickle.dump(con_mats, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIID2g15Tr0z"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SPcL-nk6Tr0z",
        "outputId": "fbde4e32-bf2f-4b4a-9731-f04c43a74ed4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f456f450-b6c8-4353-a18a-fb868d5b2b60\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TrainAccuracy</th>\n",
              "      <th>TrainLoss</th>\n",
              "      <th>TestAccuracy</th>\n",
              "      <th>TestLoss</th>\n",
              "      <th>EpochTime</th>\n",
              "      <th>Totaltime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45.2450</td>\n",
              "      <td>1.599496</td>\n",
              "      <td>55.99</td>\n",
              "      <td>1.276929</td>\n",
              "      <td>36.518333</td>\n",
              "      <td>36.518558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>62.3300</td>\n",
              "      <td>1.096489</td>\n",
              "      <td>65.49</td>\n",
              "      <td>0.986192</td>\n",
              "      <td>36.220694</td>\n",
              "      <td>72.739362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70.5925</td>\n",
              "      <td>0.869752</td>\n",
              "      <td>73.71</td>\n",
              "      <td>0.761621</td>\n",
              "      <td>35.109761</td>\n",
              "      <td>107.849230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>75.1725</td>\n",
              "      <td>0.732419</td>\n",
              "      <td>74.09</td>\n",
              "      <td>0.767701</td>\n",
              "      <td>35.598548</td>\n",
              "      <td>143.447880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>78.7875</td>\n",
              "      <td>0.633222</td>\n",
              "      <td>76.00</td>\n",
              "      <td>0.710064</td>\n",
              "      <td>35.769902</td>\n",
              "      <td>179.217909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>81.3750</td>\n",
              "      <td>0.554828</td>\n",
              "      <td>76.97</td>\n",
              "      <td>0.689467</td>\n",
              "      <td>35.135815</td>\n",
              "      <td>214.353838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>84.2950</td>\n",
              "      <td>0.467695</td>\n",
              "      <td>76.93</td>\n",
              "      <td>0.706912</td>\n",
              "      <td>35.653496</td>\n",
              "      <td>250.007441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>86.9000</td>\n",
              "      <td>0.388380</td>\n",
              "      <td>79.35</td>\n",
              "      <td>0.619393</td>\n",
              "      <td>35.859478</td>\n",
              "      <td>285.867029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>89.0325</td>\n",
              "      <td>0.327278</td>\n",
              "      <td>78.90</td>\n",
              "      <td>0.650261</td>\n",
              "      <td>35.024331</td>\n",
              "      <td>320.891469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>90.8000</td>\n",
              "      <td>0.274171</td>\n",
              "      <td>80.33</td>\n",
              "      <td>0.615199</td>\n",
              "      <td>35.478136</td>\n",
              "      <td>356.369710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>92.6975</td>\n",
              "      <td>0.219965</td>\n",
              "      <td>80.83</td>\n",
              "      <td>0.629160</td>\n",
              "      <td>35.990664</td>\n",
              "      <td>392.360518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>93.9600</td>\n",
              "      <td>0.184643</td>\n",
              "      <td>80.78</td>\n",
              "      <td>0.625300</td>\n",
              "      <td>35.220149</td>\n",
              "      <td>427.580776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>95.0600</td>\n",
              "      <td>0.150814</td>\n",
              "      <td>81.47</td>\n",
              "      <td>0.620518</td>\n",
              "      <td>35.548115</td>\n",
              "      <td>463.129006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>96.0825</td>\n",
              "      <td>0.120273</td>\n",
              "      <td>80.99</td>\n",
              "      <td>0.662590</td>\n",
              "      <td>35.889655</td>\n",
              "      <td>499.018769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>96.8575</td>\n",
              "      <td>0.100461</td>\n",
              "      <td>80.95</td>\n",
              "      <td>0.682289</td>\n",
              "      <td>35.221838</td>\n",
              "      <td>534.240713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>97.3000</td>\n",
              "      <td>0.088247</td>\n",
              "      <td>81.82</td>\n",
              "      <td>0.671472</td>\n",
              "      <td>35.550926</td>\n",
              "      <td>569.791853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>97.9575</td>\n",
              "      <td>0.068563</td>\n",
              "      <td>81.29</td>\n",
              "      <td>0.748096</td>\n",
              "      <td>35.888042</td>\n",
              "      <td>605.679998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>98.3150</td>\n",
              "      <td>0.058398</td>\n",
              "      <td>82.44</td>\n",
              "      <td>0.656213</td>\n",
              "      <td>35.056636</td>\n",
              "      <td>640.737354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>98.4600</td>\n",
              "      <td>0.053068</td>\n",
              "      <td>82.77</td>\n",
              "      <td>0.647048</td>\n",
              "      <td>34.920549</td>\n",
              "      <td>675.658029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>98.4750</td>\n",
              "      <td>0.049358</td>\n",
              "      <td>81.83</td>\n",
              "      <td>0.692126</td>\n",
              "      <td>35.004070</td>\n",
              "      <td>710.662235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>98.7250</td>\n",
              "      <td>0.044048</td>\n",
              "      <td>81.23</td>\n",
              "      <td>0.729749</td>\n",
              "      <td>34.858043</td>\n",
              "      <td>745.520383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>99.0125</td>\n",
              "      <td>0.035750</td>\n",
              "      <td>82.05</td>\n",
              "      <td>0.702947</td>\n",
              "      <td>34.956675</td>\n",
              "      <td>780.477169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>99.1475</td>\n",
              "      <td>0.031600</td>\n",
              "      <td>77.13</td>\n",
              "      <td>0.973961</td>\n",
              "      <td>34.830969</td>\n",
              "      <td>815.308250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>99.0775</td>\n",
              "      <td>0.031202</td>\n",
              "      <td>83.30</td>\n",
              "      <td>0.638305</td>\n",
              "      <td>34.839595</td>\n",
              "      <td>850.147962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>99.2925</td>\n",
              "      <td>0.027393</td>\n",
              "      <td>82.94</td>\n",
              "      <td>0.676673</td>\n",
              "      <td>35.147713</td>\n",
              "      <td>885.295783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>99.5150</td>\n",
              "      <td>0.020181</td>\n",
              "      <td>84.18</td>\n",
              "      <td>0.636640</td>\n",
              "      <td>35.416796</td>\n",
              "      <td>920.712701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>99.6900</td>\n",
              "      <td>0.013629</td>\n",
              "      <td>83.65</td>\n",
              "      <td>0.664653</td>\n",
              "      <td>35.199286</td>\n",
              "      <td>955.912090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>99.6100</td>\n",
              "      <td>0.015704</td>\n",
              "      <td>83.32</td>\n",
              "      <td>0.697796</td>\n",
              "      <td>34.769039</td>\n",
              "      <td>990.682016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>99.6150</td>\n",
              "      <td>0.014434</td>\n",
              "      <td>82.99</td>\n",
              "      <td>0.723158</td>\n",
              "      <td>34.827603</td>\n",
              "      <td>1025.509725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>99.6325</td>\n",
              "      <td>0.014703</td>\n",
              "      <td>82.37</td>\n",
              "      <td>0.733292</td>\n",
              "      <td>34.893943</td>\n",
              "      <td>1060.403791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>99.6925</td>\n",
              "      <td>0.012916</td>\n",
              "      <td>82.99</td>\n",
              "      <td>0.708368</td>\n",
              "      <td>35.078665</td>\n",
              "      <td>1095.482571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>99.6625</td>\n",
              "      <td>0.014143</td>\n",
              "      <td>83.40</td>\n",
              "      <td>0.686280</td>\n",
              "      <td>34.907257</td>\n",
              "      <td>1130.389946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>99.7950</td>\n",
              "      <td>0.010037</td>\n",
              "      <td>84.12</td>\n",
              "      <td>0.678465</td>\n",
              "      <td>34.939761</td>\n",
              "      <td>1165.329819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>99.8575</td>\n",
              "      <td>0.007296</td>\n",
              "      <td>84.07</td>\n",
              "      <td>0.672648</td>\n",
              "      <td>34.970215</td>\n",
              "      <td>1200.300138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>99.8325</td>\n",
              "      <td>0.008279</td>\n",
              "      <td>82.46</td>\n",
              "      <td>0.768197</td>\n",
              "      <td>35.274675</td>\n",
              "      <td>1235.574925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>99.7800</td>\n",
              "      <td>0.008908</td>\n",
              "      <td>84.17</td>\n",
              "      <td>0.671870</td>\n",
              "      <td>35.347537</td>\n",
              "      <td>1270.922578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>99.8300</td>\n",
              "      <td>0.008122</td>\n",
              "      <td>83.55</td>\n",
              "      <td>0.705952</td>\n",
              "      <td>35.035747</td>\n",
              "      <td>1305.958455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>99.8525</td>\n",
              "      <td>0.007037</td>\n",
              "      <td>84.00</td>\n",
              "      <td>0.701952</td>\n",
              "      <td>34.757108</td>\n",
              "      <td>1340.715671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>99.8825</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>84.11</td>\n",
              "      <td>0.683053</td>\n",
              "      <td>35.130770</td>\n",
              "      <td>1375.846574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>99.9225</td>\n",
              "      <td>0.004951</td>\n",
              "      <td>83.86</td>\n",
              "      <td>0.709730</td>\n",
              "      <td>34.882228</td>\n",
              "      <td>1410.728923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>99.9300</td>\n",
              "      <td>0.004667</td>\n",
              "      <td>83.93</td>\n",
              "      <td>0.703531</td>\n",
              "      <td>34.846112</td>\n",
              "      <td>1445.575162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>99.9300</td>\n",
              "      <td>0.004104</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.712221</td>\n",
              "      <td>34.848665</td>\n",
              "      <td>1480.424326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>99.9525</td>\n",
              "      <td>0.003764</td>\n",
              "      <td>84.11</td>\n",
              "      <td>0.716918</td>\n",
              "      <td>35.075142</td>\n",
              "      <td>1515.499602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>99.9675</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>84.16</td>\n",
              "      <td>0.705925</td>\n",
              "      <td>35.093869</td>\n",
              "      <td>1550.593599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>99.9575</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>84.34</td>\n",
              "      <td>0.698732</td>\n",
              "      <td>35.151559</td>\n",
              "      <td>1585.745269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>99.9600</td>\n",
              "      <td>0.002880</td>\n",
              "      <td>84.23</td>\n",
              "      <td>0.715842</td>\n",
              "      <td>35.016929</td>\n",
              "      <td>1620.763052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>99.9300</td>\n",
              "      <td>0.004065</td>\n",
              "      <td>83.40</td>\n",
              "      <td>0.740705</td>\n",
              "      <td>34.841192</td>\n",
              "      <td>1655.604356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>99.9400</td>\n",
              "      <td>0.003858</td>\n",
              "      <td>83.84</td>\n",
              "      <td>0.720828</td>\n",
              "      <td>34.791316</td>\n",
              "      <td>1690.395786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>99.9175</td>\n",
              "      <td>0.004060</td>\n",
              "      <td>83.99</td>\n",
              "      <td>0.711047</td>\n",
              "      <td>34.981257</td>\n",
              "      <td>1725.377171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>99.8700</td>\n",
              "      <td>0.005772</td>\n",
              "      <td>84.11</td>\n",
              "      <td>0.731200</td>\n",
              "      <td>34.865556</td>\n",
              "      <td>1760.242846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>99.9075</td>\n",
              "      <td>0.004253</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.741448</td>\n",
              "      <td>34.925526</td>\n",
              "      <td>1795.168484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>99.8200</td>\n",
              "      <td>0.007283</td>\n",
              "      <td>83.33</td>\n",
              "      <td>0.743372</td>\n",
              "      <td>34.873909</td>\n",
              "      <td>1830.042511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>99.8950</td>\n",
              "      <td>0.005063</td>\n",
              "      <td>83.98</td>\n",
              "      <td>0.726381</td>\n",
              "      <td>35.299299</td>\n",
              "      <td>1865.341923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>99.9350</td>\n",
              "      <td>0.003791</td>\n",
              "      <td>83.59</td>\n",
              "      <td>0.736539</td>\n",
              "      <td>35.444895</td>\n",
              "      <td>1900.786934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>99.8900</td>\n",
              "      <td>0.004777</td>\n",
              "      <td>83.75</td>\n",
              "      <td>0.732799</td>\n",
              "      <td>35.367693</td>\n",
              "      <td>1936.154728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>99.9400</td>\n",
              "      <td>0.003270</td>\n",
              "      <td>84.17</td>\n",
              "      <td>0.710532</td>\n",
              "      <td>34.880422</td>\n",
              "      <td>1971.035884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>99.9500</td>\n",
              "      <td>0.003062</td>\n",
              "      <td>84.20</td>\n",
              "      <td>0.705114</td>\n",
              "      <td>34.837490</td>\n",
              "      <td>2005.873501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>99.9625</td>\n",
              "      <td>0.002978</td>\n",
              "      <td>84.24</td>\n",
              "      <td>0.720443</td>\n",
              "      <td>34.880688</td>\n",
              "      <td>2040.754293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>99.9400</td>\n",
              "      <td>0.003122</td>\n",
              "      <td>83.98</td>\n",
              "      <td>0.723768</td>\n",
              "      <td>34.911653</td>\n",
              "      <td>2075.666055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>99.9225</td>\n",
              "      <td>0.003570</td>\n",
              "      <td>83.59</td>\n",
              "      <td>0.712926</td>\n",
              "      <td>35.028680</td>\n",
              "      <td>2110.694843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>99.9275</td>\n",
              "      <td>0.003722</td>\n",
              "      <td>83.79</td>\n",
              "      <td>0.736949</td>\n",
              "      <td>35.002554</td>\n",
              "      <td>2145.697526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>99.9700</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>84.50</td>\n",
              "      <td>0.709608</td>\n",
              "      <td>34.937665</td>\n",
              "      <td>2180.635302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>99.9850</td>\n",
              "      <td>0.001836</td>\n",
              "      <td>84.58</td>\n",
              "      <td>0.684052</td>\n",
              "      <td>35.118435</td>\n",
              "      <td>2215.753836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>99.9800</td>\n",
              "      <td>0.001663</td>\n",
              "      <td>84.47</td>\n",
              "      <td>0.704153</td>\n",
              "      <td>35.389200</td>\n",
              "      <td>2251.143168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>99.9825</td>\n",
              "      <td>0.001587</td>\n",
              "      <td>84.42</td>\n",
              "      <td>0.703223</td>\n",
              "      <td>35.034797</td>\n",
              "      <td>2286.178089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>99.9675</td>\n",
              "      <td>0.001857</td>\n",
              "      <td>84.38</td>\n",
              "      <td>0.708134</td>\n",
              "      <td>34.725466</td>\n",
              "      <td>2320.903661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>99.9800</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>83.97</td>\n",
              "      <td>0.727068</td>\n",
              "      <td>34.975301</td>\n",
              "      <td>2355.879078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>99.9650</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>84.04</td>\n",
              "      <td>0.724879</td>\n",
              "      <td>35.127352</td>\n",
              "      <td>2391.006547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>99.9650</td>\n",
              "      <td>0.001984</td>\n",
              "      <td>84.38</td>\n",
              "      <td>0.717296</td>\n",
              "      <td>34.970077</td>\n",
              "      <td>2425.976731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>99.9125</td>\n",
              "      <td>0.003243</td>\n",
              "      <td>84.71</td>\n",
              "      <td>0.698396</td>\n",
              "      <td>34.935353</td>\n",
              "      <td>2460.912200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>99.9400</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>83.66</td>\n",
              "      <td>0.754767</td>\n",
              "      <td>35.031279</td>\n",
              "      <td>2495.943604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>99.9650</td>\n",
              "      <td>0.002240</td>\n",
              "      <td>84.09</td>\n",
              "      <td>0.738799</td>\n",
              "      <td>35.175538</td>\n",
              "      <td>2531.119252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>99.9750</td>\n",
              "      <td>0.002048</td>\n",
              "      <td>84.32</td>\n",
              "      <td>0.719483</td>\n",
              "      <td>35.305032</td>\n",
              "      <td>2566.424402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>99.9700</td>\n",
              "      <td>0.001939</td>\n",
              "      <td>84.36</td>\n",
              "      <td>0.738790</td>\n",
              "      <td>35.306742</td>\n",
              "      <td>2601.731262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>99.9775</td>\n",
              "      <td>0.001514</td>\n",
              "      <td>84.51</td>\n",
              "      <td>0.715296</td>\n",
              "      <td>34.989381</td>\n",
              "      <td>2636.721527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>99.9750</td>\n",
              "      <td>0.001567</td>\n",
              "      <td>84.19</td>\n",
              "      <td>0.743778</td>\n",
              "      <td>34.854351</td>\n",
              "      <td>2671.575988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>99.9925</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>84.58</td>\n",
              "      <td>0.717279</td>\n",
              "      <td>34.964258</td>\n",
              "      <td>2706.540358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>99.9925</td>\n",
              "      <td>0.001113</td>\n",
              "      <td>84.58</td>\n",
              "      <td>0.718956</td>\n",
              "      <td>34.895602</td>\n",
              "      <td>2741.436080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>99.9725</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>84.35</td>\n",
              "      <td>0.757594</td>\n",
              "      <td>34.889743</td>\n",
              "      <td>2776.325936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>99.9625</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>84.04</td>\n",
              "      <td>0.742996</td>\n",
              "      <td>34.959959</td>\n",
              "      <td>2811.286012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f456f450-b6c8-4353-a18a-fb868d5b2b60')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f456f450-b6c8-4353-a18a-fb868d5b2b60 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f456f450-b6c8-4353-a18a-fb868d5b2b60');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e1e68a90-135b-47d8-8379-eab85cd2f994\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e1e68a90-135b-47d8-8379-eab85cd2f994')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e1e68a90-135b-47d8-8379-eab85cd2f994 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    TrainAccuracy  TrainLoss  TestAccuracy  TestLoss  EpochTime    Totaltime\n",
              "0         45.2450   1.599496         55.99  1.276929  36.518333    36.518558\n",
              "1         62.3300   1.096489         65.49  0.986192  36.220694    72.739362\n",
              "2         70.5925   0.869752         73.71  0.761621  35.109761   107.849230\n",
              "3         75.1725   0.732419         74.09  0.767701  35.598548   143.447880\n",
              "4         78.7875   0.633222         76.00  0.710064  35.769902   179.217909\n",
              "5         81.3750   0.554828         76.97  0.689467  35.135815   214.353838\n",
              "6         84.2950   0.467695         76.93  0.706912  35.653496   250.007441\n",
              "7         86.9000   0.388380         79.35  0.619393  35.859478   285.867029\n",
              "8         89.0325   0.327278         78.90  0.650261  35.024331   320.891469\n",
              "9         90.8000   0.274171         80.33  0.615199  35.478136   356.369710\n",
              "10        92.6975   0.219965         80.83  0.629160  35.990664   392.360518\n",
              "11        93.9600   0.184643         80.78  0.625300  35.220149   427.580776\n",
              "12        95.0600   0.150814         81.47  0.620518  35.548115   463.129006\n",
              "13        96.0825   0.120273         80.99  0.662590  35.889655   499.018769\n",
              "14        96.8575   0.100461         80.95  0.682289  35.221838   534.240713\n",
              "15        97.3000   0.088247         81.82  0.671472  35.550926   569.791853\n",
              "16        97.9575   0.068563         81.29  0.748096  35.888042   605.679998\n",
              "17        98.3150   0.058398         82.44  0.656213  35.056636   640.737354\n",
              "18        98.4600   0.053068         82.77  0.647048  34.920549   675.658029\n",
              "19        98.4750   0.049358         81.83  0.692126  35.004070   710.662235\n",
              "20        98.7250   0.044048         81.23  0.729749  34.858043   745.520383\n",
              "21        99.0125   0.035750         82.05  0.702947  34.956675   780.477169\n",
              "22        99.1475   0.031600         77.13  0.973961  34.830969   815.308250\n",
              "23        99.0775   0.031202         83.30  0.638305  34.839595   850.147962\n",
              "24        99.2925   0.027393         82.94  0.676673  35.147713   885.295783\n",
              "25        99.5150   0.020181         84.18  0.636640  35.416796   920.712701\n",
              "26        99.6900   0.013629         83.65  0.664653  35.199286   955.912090\n",
              "27        99.6100   0.015704         83.32  0.697796  34.769039   990.682016\n",
              "28        99.6150   0.014434         82.99  0.723158  34.827603  1025.509725\n",
              "29        99.6325   0.014703         82.37  0.733292  34.893943  1060.403791\n",
              "30        99.6925   0.012916         82.99  0.708368  35.078665  1095.482571\n",
              "31        99.6625   0.014143         83.40  0.686280  34.907257  1130.389946\n",
              "32        99.7950   0.010037         84.12  0.678465  34.939761  1165.329819\n",
              "33        99.8575   0.007296         84.07  0.672648  34.970215  1200.300138\n",
              "34        99.8325   0.008279         82.46  0.768197  35.274675  1235.574925\n",
              "35        99.7800   0.008908         84.17  0.671870  35.347537  1270.922578\n",
              "36        99.8300   0.008122         83.55  0.705952  35.035747  1305.958455\n",
              "37        99.8525   0.007037         84.00  0.701952  34.757108  1340.715671\n",
              "38        99.8825   0.005814         84.11  0.683053  35.130770  1375.846574\n",
              "39        99.9225   0.004951         83.86  0.709730  34.882228  1410.728923\n",
              "40        99.9300   0.004667         83.93  0.703531  34.846112  1445.575162\n",
              "41        99.9300   0.004104         83.71  0.712221  34.848665  1480.424326\n",
              "42        99.9525   0.003764         84.11  0.716918  35.075142  1515.499602\n",
              "43        99.9675   0.003108         84.16  0.705925  35.093869  1550.593599\n",
              "44        99.9575   0.003642         84.34  0.698732  35.151559  1585.745269\n",
              "45        99.9600   0.002880         84.23  0.715842  35.016929  1620.763052\n",
              "46        99.9300   0.004065         83.40  0.740705  34.841192  1655.604356\n",
              "47        99.9400   0.003858         83.84  0.720828  34.791316  1690.395786\n",
              "48        99.9175   0.004060         83.99  0.711047  34.981257  1725.377171\n",
              "49        99.8700   0.005772         84.11  0.731200  34.865556  1760.242846\n",
              "50        99.9075   0.004253         83.71  0.741448  34.925526  1795.168484\n",
              "51        99.8200   0.007283         83.33  0.743372  34.873909  1830.042511\n",
              "52        99.8950   0.005063         83.98  0.726381  35.299299  1865.341923\n",
              "53        99.9350   0.003791         83.59  0.736539  35.444895  1900.786934\n",
              "54        99.8900   0.004777         83.75  0.732799  35.367693  1936.154728\n",
              "55        99.9400   0.003270         84.17  0.710532  34.880422  1971.035884\n",
              "56        99.9500   0.003062         84.20  0.705114  34.837490  2005.873501\n",
              "57        99.9625   0.002978         84.24  0.720443  34.880688  2040.754293\n",
              "58        99.9400   0.003122         83.98  0.723768  34.911653  2075.666055\n",
              "59        99.9225   0.003570         83.59  0.712926  35.028680  2110.694843\n",
              "60        99.9275   0.003722         83.79  0.736949  35.002554  2145.697526\n",
              "61        99.9700   0.002493         84.50  0.709608  34.937665  2180.635302\n",
              "62        99.9850   0.001836         84.58  0.684052  35.118435  2215.753836\n",
              "63        99.9800   0.001663         84.47  0.704153  35.389200  2251.143168\n",
              "64        99.9825   0.001587         84.42  0.703223  35.034797  2286.178089\n",
              "65        99.9675   0.001857         84.38  0.708134  34.725466  2320.903661\n",
              "66        99.9800   0.001590         83.97  0.727068  34.975301  2355.879078\n",
              "67        99.9650   0.001934         84.04  0.724879  35.127352  2391.006547\n",
              "68        99.9650   0.001984         84.38  0.717296  34.970077  2425.976731\n",
              "69        99.9125   0.003243         84.71  0.698396  34.935353  2460.912200\n",
              "70        99.9400   0.002670         83.66  0.754767  35.031279  2495.943604\n",
              "71        99.9650   0.002240         84.09  0.738799  35.175538  2531.119252\n",
              "72        99.9750   0.002048         84.32  0.719483  35.305032  2566.424402\n",
              "73        99.9700   0.001939         84.36  0.738790  35.306742  2601.731262\n",
              "74        99.9775   0.001514         84.51  0.715296  34.989381  2636.721527\n",
              "75        99.9750   0.001567         84.19  0.743778  34.854351  2671.575988\n",
              "76        99.9925   0.001116         84.58  0.717279  34.964258  2706.540358\n",
              "77        99.9925   0.001113         84.58  0.718956  34.895602  2741.436080\n",
              "78        99.9725   0.001498         84.35  0.757594  34.889743  2776.325936\n",
              "79        99.9625   0.001830         84.04  0.742996  34.959959  2811.286012"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "df_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K-r5RE5kOft"
      },
      "source": [
        "### Best Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHGK9EWrjvvx",
        "outputId": "7abd03ac-dfec-4552-c65d-adcf3fe5fe35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest accuracy reached: 84.71%\n"
          ]
        }
      ],
      "source": [
        "max_accuracy = df_res['TestAccuracy'].max()\n",
        "print(f\"Highest accuracy reached: {max_accuracy}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
